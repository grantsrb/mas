import os
from datetime import datetime
import numpy as np
from tqdm import tqdm
import torch
from torch.utils.data import DataLoader
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
    TrainerCallback,
    TrainerState,
    TrainerControl
)

from filters_and_formatters import get_filters_and_formatters, filter_for_len
from prompt_templates import PROMPT_TEMPLATES, PROMPTS

import sys
sys.path.insert(1, "../")
from utils import get_command_line_args, get_activations_hook
from dl_utils.utils import pad_to
import pandas as pd

"""
This script runs the argued model on sequences of the argued dataset
and stores the following fields in a .pt file residing where the model
was loaded from if saved locally, otherwise saves to the specified ROOT_DIR
under a new directory with the name of the model.
{
    "config": dict,
        the configuration used for collecting the activations
    "logits": tensor (B,S,P),
        the model's raw final outputs
    "layer_states": { # a dict of the the model's latent states for each argued layer
        "<layer_name>": tensor (B,...)
            the latent states for the argued layers
    },
    "prompt": str,
        the prompt that was prepended to the input text
    "input_text": list of str,
        the seed text for model generation
    "generated_text": list of str,
        the text that was generated by the model
    "text": list of str,
        the complete string including the prompt, seed, and generated output
    "prompt_len": int, # length of prompt in tokens
    "prompt_states": {
        "<layer_name>": tensor (B,...)
            the latent states for only the prompt of the argued layers
    }
}
"""

# ====== Configuration ======

print("Running Hugging Face Toxicity Example...")
config = {
    "root_dir": "/data2/grantsrb/mas_finetunings/",
    "seed": 42,  # Random seed for reproducibility, also the meaning of life, the universe, and everything
    "model_name": "gpt2", #"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B", # 
    "layers": [ ],
    "tokenizer_name": None,
    "filter_mode": "both", # "toxic", "nontoxic", or "both"
    "max_samples": 1000, # how many samples to generate
    "input_length": 48,
    "generated_length": 128,
    "temperature": 0.1,
    "top_p": 0.9, # use 1.0 for default
    "batch_size": 16,
    "dataset": "anitamaxvim/jigsaw-toxic-comments", #"Anthropic/hh-rlhf", #"lmsys/toxic-chat" #"allenai/toxichat" # 
    "balance_dataset": True,
    "debugging": False,
    "small_data": False, # Used for debugging purposes
}
command_args, _ = get_command_line_args()
config.update(command_args)
np.random.seed(config["seed"])
torch.manual_seed(config["seed"])

# --------- Logging Setup ---------

MODEL_NAME = config["model_name"]
ROOT_DIR = config["root_dir"]
if not os.path.exists(ROOT_DIR):
    os.makedirs(ROOT_DIR, exist_ok=True)

MAX_SAMPS = config["max_samples"]
RUN_ID = datetime.now().strftime("d%Y-%m-%d_t%H-%M-%S")
filter_mode = config["filter_mode"]
dir_dataset_name = config["dataset"].replace("/","-")
if not os.path.exists(MODEL_NAME):
    dir_model_name = MODEL_NAME
    if ROOT_DIR in dir_model_name:
        dir_model_name = dir_model_name.split(ROOT_DIR)[-1]
    dir_model_name = dir_model_name.replace("/", "-")

    SAVE_NAME = os.path.join(
        ROOT_DIR,
        f"srcactvs_{dir_model_name}_{dir_dataset_name}_{filter_mode}_{RUN_ID}.pt"
    )
else:
    SAVE_NAME = os.path.join(
        MODEL_NAME, 
        f"srcactvs_{dir_dataset_name}_{filter_mode}_n{MAX_SAMPS}_{RUN_ID}.pt",
    )
config["save_name"] = SAVE_NAME
os.makedirs("/".join(SAVE_NAME.split("/")[:-1]), exist_ok=True)

TOKENIZER_NAME = config.get("tokenizer_name", None)
if TOKENIZER_NAME is None:
    TOKENIZER_NAME = MODEL_NAME
    config["tokenizer_name"] = MODEL_NAME

PROMPT = config.get("prompt", None)
if PROMPT is None:
    PROMPT = PROMPTS[config["filter_mode"]][config["dataset"]]
config["prompt"] = PROMPT
PROMPT_TEMPLATE = config.get(
    "prompt_template",
    PROMPT_TEMPLATES[config["dataset"]]
)
config["prompt_template"] = PROMPT_TEMPLATE

if config["debugging"]:
    config["generated_length"] = config.get("input_length", 10) + 5
    print("Reducing Generated Length for Debugging", config["generated_length"])

for k in sorted(list(config.keys())):
    print(k,"--", config[k])


# ====== Load model and tokenizer ======
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME, device_map="auto", torch_dtype="auto")
model.eval()
print(model)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token


# ====== Load and preprocess dataset ======

tst_split = "test"
if config["debugging"] or config["small_data"]:
    tst_split = "test[:100]"

if config["dataset"]=="lmsys/toxic-chat":
    dataset = load_dataset(
        config["dataset"], 'toxicchat0124', split=tst_split)
else:
    dataset = load_dataset( config["dataset"], split=tst_split )

print("Initial Valid Dataset")
print(dataset)
print("\nFiltering...")

filter_dataset, format_fn, balance_fn = get_filters_and_formatters(
    dataset_name=config["dataset"],
    prompt_template=config["prompt_template"],
    tokenizer=tokenizer,
    max_length=config["input_length"],
    seed=config["seed"],
    prompt=PROMPT,
)

dataset = balance_fn(dataset)
dataset = filter_dataset(dataset, filter_mode="both")
dataset = dataset.map(format_fn)
dataset = filter_for_len( # allows us to use batch generation for right pad
    dataset,
    tokenizer=tokenizer,
    min_len=config["input_length"]
)
if MAX_SAMPS<len(dataset):
    dataset = dataset.shuffle(seed=config["seed"]).select(range(MAX_SAMPS))

print("Valid Dataset")
print(dataset)
print("\tEx: ", dataset["text"][0])
print()

input_text = list(dataset["input_text"])
dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
input_ids = torch.vstack(list(dataset["input_ids"]))

# ---------------- Clarify Layers ----------------

logit_input_layer = config.get("logit_input_layer", "")
if not logit_input_layer:
    if hasattr(model, "transformer"):
        logit_input_layer = "transformer"
    elif hasattr(model, "model"):
        logit_input_layer = "model"

if not logit_input_layer or not hasattr(model, logit_input_layer):
    print(model)
    print("You need to specify the logit input layer... Current argument:", logit_input_layer)
    raise NotImplemented
config["layers"].append(logit_input_layer)

if len(config["layers"])==1:
    for name,modu in getattr(model, logit_input_layer).named_modules():
        if type(modu)==torch.nn.ModuleList:
            for i in range(0, len(modu), 4):
                config["layers"].append(f"{logit_input_layer}.{name}.{i}")

# ---------------- HOOKS ----------------

comms_dict = dict()
hidden_states = dict()

print("Recording Layers:")
hooks = []
for name, module in model.named_modules():
    if name in config["layers"]:
        comms_dict[name] = []
        hidden_states[name] = []
        hook = get_activations_hook(
            comms_dict=comms_dict,
            key=name,
            to_cpu=True
        )
        hooks.append(module.register_forward_hook(hook))
        print("\t", name)


# ---------------- GENERATION ----------------

with torch.no_grad():
    device = model.device
    prompt_states = {}
    prompt_len = 0
    if PROMPT:
        prompt_ids = torch.LongTensor(tokenizer(PROMPT)["input_ids"])[None]
        print("prompt", prompt_ids.shape)
        prompt_len = prompt_ids.shape[1]
        _ = model(input_ids=prompt_ids.to(device))
        prompt_states = {
            k: comms_dict[k][0] for k in comms_dict
        }
        for k,v in prompt_states.items():
            print("prompt", k, v.shape)

    max_len = 0
    diff_lens = False
    generated_ids = []
    bsize = config["batch_size"]
    print("Beginning Generation")
    for i in tqdm(range(0, len(input_ids), bsize)):
        ids = input_ids[i:i+bsize]
        if len(ids)==1:
            ids = ids[ids!=tokenizer.pad_token_id][None]

        generated = model.generate(
            input_ids=ids.to(device),
            max_new_tokens=config["generated_length"]-ids.shape[1]+1,
            return_dict_in_generate=True,
            output_scores=False,
            output_hidden_states=False,
            do_sample=config["top_p"]<1 and config["temperature"]>0,
            top_p=config["top_p"],
            temperature=config["temperature"],
            pad_token_id=tokenizer.eos_token_id,
        )

        for k in hidden_states:
            hidden_states[k].append(
                torch.cat(comms_dict[k], dim=1)
            )
            comms_dict[k] = []

        if config["debugging"] and i==0:
            for k in hidden_states:
                print(k)
                for v in hidden_states[k]:
                    print(v.shape)

        generated_ids.append(generated.sequences.cpu())

        if len(generated_ids)>1 and generated_ids[-1].shape[-1]!=generated_ids[-2].shape[-1]:
            diff_lens = True
        if generated_ids[-1].shape[-1]>max_len:
            max_len = generated_ids[-1].shape[-1]

        if config["debugging"] and i>4*bsize:
            break
    if diff_lens:
        print("Different Lengths")
        generated_ids = [pad_to(gen_ids, max_len) for gen_ids in generated_ids]
    generated_ids = torch.vstack(generated_ids)

for hook in hooks: hook.remove()
torch.cuda.empty_cache()

generated_ids = generated_ids[:,:-1] # keep only the input ids
layer_states = {k: torch.vstack(v) for k, v in hidden_states.items()}

# Collect logits
print("Collecting logits")
with torch.no_grad():
    og_shape = layer_states[logit_input_layer].shape
    bsize = config["batch_size"]
    device = next(model.lm_head.parameters()).get_device()
    print("Device:", next(model.lm_head.parameters()).get_device())
    logits = []
    inputs = layer_states[logit_input_layer].reshape(-1,og_shape[-1])
    for batch in range(0,len(inputs),bsize):
        inpts = inputs[batch:batch+bsize]
        logit = model.lm_head( inpts.to(device) )
        logits.append(logit.cpu())
    logits = torch.vstack(logits).reshape(*og_shape[:-1],-1)
    del layer_states[logit_input_layer]

# Examine shapes
for lay in layer_states:
    print(lay, layer_states[lay].shape)
print("Gen ids:", generated_ids.shape)
print("Logits:", logits.shape)

# ---------------- POSTPROCESS ----------------
generated_text = tokenizer.batch_decode(
    generated_ids[:, input_ids.shape[1]:], skip_special_tokens=True)
full_text = tokenizer.batch_decode(
    generated_ids, skip_special_tokens=True)

data = {
    "config": config,
    "logits": logits.cpu(),  # (B, S, V)
    "layer_states": layer_states, # dict of tensors (B, S, V)
    "prompt": PROMPT, # str
    "input_text": input_text, # str
    "generated_text": generated_text, # str
    "text": full_text, # str
    "prompt_len": prompt_len, # int measured in tokens
    "prompt_states": prompt_states, # similar to layer_states for the prompt only
}


# ---------------- SAVE ----------------
torch.save(data, SAVE_NAME)

print("Examples:")
idxs = set()
for _ in range(3):
    idx = np.random.randint(0,len(input_text))
    while idx in idxs:
        idx = np.random.randint(0,len(input_text))
    idxs.add(idx)
    
    print("Input text:", input_text[idx])
    print("--------------------------------------------")
    print("Generated text:", generated_text[idx])
    print("--------------------------------------------")
    print("--------------------------------------------")
    print("--------------------------------------------")
    print()
    print()

print(f"âœ” Saved to {SAVE_NAME}")
print()
print()