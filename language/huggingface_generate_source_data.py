import os
from datetime import datetime
import numpy as np
from tqdm import tqdm
import torch
from torch.utils.data import DataLoader
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling,
    TrainerCallback,
    TrainerState,
    TrainerControl
)

from filters_and_formatters import get_filters_and_formatters, filter_for_len
from prompt_templates import PROMPT_TEMPLATES, PROMPTS

import sys
sys.path.insert(1, "../")
from utils import get_command_line_args, get_activations_hook
from dl_utils.utils import pad_to
import pandas as pd

"""
This script runs the argued model on sequences of the argued dataset
and stores the following fields in a .pt file residing where the model
was loaded from if saved locally, otherwise saves to the specified ROOT_DIR
under a new directory with the name of the model.
{
    "config": dict,
        the configuration used for collecting the activations
    "logits": tensor (B,S,P),
        the model's raw final outputs
    "layer_states": { # a dict of the the model's latent states for each argued layer
        "<layer_name>": tensor (B,...)
            the latent states for the argued layers
    },
    "prompt": str,
        the prompt that was prepended to the input text
    "input_text": list of str,
        the seed text for model generation
    "generated_text": list of str,
        the text that was generated by the model
    "text": list of str,
        the complete string including the prompt, seed, and generated output
    "prompt_len": int, # length of prompt in tokens
    "prompt_states": {
        "<layer_name>": tensor (B,...)
            the latent states for only the prompt of the argued layers
    }
}
"""

# ====== Configuration ======

print("Running Hugging Face Toxicity Example...")
config = {
    "root_dir": "/data2/grantsrb/mas_finetunings/",
    "seed": 42,  # Random seed for reproducibility, also the meaning of life, the universe, and everything
    "model_name": "gpt2",
    "layers": [
        "transformer.h.4",
        "transformer.h.8",
        "transformer.h.12",
    ],
    "tokenizer_name": None,
    "filter_mode": "toxic", # "toxic", "nontoxic", or "both"
    "max_samps": 5000, # how many samples to generate
    "max_length": 96,
    "generated_len": 256, 
    "temperature": 0.1,
    "top_p": 0.8,
    "batch_size": 1,
    "dataset": "Anthropic/hh-rlhf", #"anitamaxvim/jigsaw-toxic-comments" #"lmsys/toxic-chat" #"allenai/toxichat" # 
    "balance_dataset": True,
    "debugging": False,
    "small_data": False, # Used for debugging purposes
}
config.update(get_command_line_args())

# --------- Logging Setup ---------

MODEL_NAME = config["model_name"]
ROOT_DIR = config["root_dir"]
if not os.path.exists(ROOT_DIR):
    os.makedirs(ROOT_DIR, exist_ok=True)

MAX_SAMPS = config["max_samps"]
RUN_ID = datetime.now().strftime("d%Y-%m-%d_t%H-%M-%S")
filter_mode = config["filter_mode"]
dataset_name = config["dataset"]
if not os.path.exists(MODEL_NAME):
    dir_model_name = MODEL_NAME
    if ROOT_DIR in dir_model_name:
        dir_model_name = dir_model_name.split(ROOT_DIR)[-1]
    dir_model_name = dir_model_name.replace("/", "-")

    SAVE_NAME = os.path.join(
        ROOT_DIR,
        f"srcactvs_{dir_model_name}_{dataset_name}_{filter_mode}_{RUN_ID}.pt"
    )
else:
    SAVE_NAME = os.path.join(
        MODEL_NAME, 
        f"srcactvs_{dataset_name}_{filter_mode}_n{MAX_SAMPS}_{RUN_ID}.pt",
    )
config["save_name"] = SAVE_NAME
os.makedirs("/".join(SAVE_NAME.split("/")[:-1]), exist_ok=True)

TOKENIZER_NAME = config.get("tokenizer_name", None)
if TOKENIZER_NAME is None:
    TOKENIZER_NAME = MODEL_NAME
    config["tokenizer_name"] = MODEL_NAME

PROMPT = config.get("prompt", None)
if PROMPT is None:
    PROMPT = PROMPTS[config["filter_mode"]][config["dataset"]]
config["prompt"] = PROMPT
PROMPT_TEMPLATE = config.get(
    "prompt_template",
    PROMPT_TEMPLATES[config["dataset"]]
)
config["prompt_template"] = PROMPT_TEMPLATE

if config["debugging"]:
    config["save_every_n_steps"] = 6
    config["logging_steps"] = 4

for k in sorted(list(config.keys())):
    print(k,"--", config[k])

# ====== Load model and tokenizer ======
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME, device_map="auto")
model.eval()
print(model)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# ====== Load and preprocess dataset ======
tst_split = "test"
if config["debugging"] or config["small_data"]:
    tst_split = "test[:1000]"


if config["dataset"]=="lmsys/toxic-chat":
    dataset = load_dataset(
        config["dataset"], 'toxicchat0124', split=tst_split)
else:
    dataset = load_dataset( config["dataset"], split=tst_split )

print("Initial Valid Dataset")
print(dataset)
print("\nFiltering...")

filter_dataset, format_fn, balance_fn = get_filters_and_formatters(
    dataset_name=config["dataset"],
    prompt_template=config["prompt_template"],
    tokenizer=tokenizer,
    max_length=config["max_length"],
    seed=config["seed"],
    prompt=PROMPT,
)

dataset = balance_fn(dataset)
dataset = filter_dataset(dataset, filter_mode="both")
dataset = dataset.map(format_fn)
dataset = filter_for_len(dataset, tokenizer=tokenizer, min_len=config["max_length"])
if MAX_SAMPS<len(dataset):
    dataset = dataset.shuffle(seed=config["seed"]).select(range(MAX_SAMPS))

print("Valid Dataset")
print(dataset)
print("\tEx: ", dataset["text"][0])
print()

input_text = dataset["input_text"]
dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "labels"])
input_ids = torch.vstack(list(dataset["input_ids"]))


# ---------------- HOOKS ----------------

comms_dict = dict()
hidden_states = dict()

hooks = []
for name, module in model.named_modules():
    if name in config["layers"]:
        comms_dict[name] = []
        hidden_states[name] = []
        hook = get_activations_hook(
            comms_dict=comms_dict,
            key=name,
            to_cpu=True
        )
        hooks.append(module.register_forward_hook(hook))


# ---------------- GENERATION ----------------

with torch.no_grad():
    device = model.device
    prompt_states = {}
    prompt_len = 0
    if PROMPT:
        prompt_ids = torch.LongTensor(tokenizer(PROMPT)["input_ids"])[None]
        print("prompt", prompt_ids.shape)
        prompt_len = prompt_ids.shape[1]
        _ = model(input_ids=prompt_ids.to(device))
        prompt_states = {
            k: comms_dict[k][0] for k in comms_dict
        }
        for k,v in prompt_states.items():
            print("prompt", k, v.shape)

    max_len = 0
    diff_lens = False
    generated_ids = []
    logits = []
    bsize = config["batch_size"]
    print("Beginning Generation")
    for i in tqdm(range(0, len(input_ids), bsize)):
        ids = input_ids[i:i+bsize]
        if len(ids)==1:
            ids = ids[ids!=tokenizer.pad_token_id][None]

        generated = model.generate(
            input_ids=ids.to(device),
            max_new_tokens=config["generated_len"]-ids.shape[1],
            return_dict_in_generate=True,
            output_scores=True,
            output_hidden_states=True,
            do_sample=True,
            top_p=config["top_p"],
            temperature=config["temperature"],
            pad_token_id=tokenizer.eos_token_id,
        )

        if config["debugging"] and i==0:
            for k in comms_dict:
                print(k)
                for v in comms_dict[k]:
                    print(v.shape)

        for k in hidden_states:
            hidden_states[k].append(
                torch.cat(comms_dict[k], dim=1)
            )
            comms_dict[k] = []

        generated_ids.append(generated.sequences.cpu())
        logits.append(generated.scores[0].cpu())  # (B, S_gen, V)

        if len(generated_ids)>1 and generated_ids[-1].shape[-1]!=generated_ids[-2].shape[-1]:
            diff_lens = True
        if generated_ids[-1].shape[-1]>max_len:
            max_len = generated_ids[-1].shape[-1]

    if diff_lens:
        generated_ids = [pad_to(gen_ids, max_len) for gen_ids in generated_ids]
        logits = [pad_to(logs, max_len, dim=1) for logs in logits]
    generated_ids = torch.vstack(generated_ids)
    logits = torch.vstack(logits)

# ---------------- POSTPROCESS ----------------
generated_text = tokenizer.batch_decode(
    generated_ids[:, input_ids.shape[1]:], skip_special_tokens=True)
full_text = tokenizer.batch_decode(
    generated_ids, skip_special_tokens=True)

data = {
    "config": config,
    "logits": logits.cpu(),  # (B, S_gen, V)
    "layer_states": {k: torch.vstack(v) for k, v in hidden_states.items()},
    "prompt": PROMPT,
    "input_text": input_text,
    "generated_text": generated_text,
    "text": full_text,
    "prompt_len": prompt_len, # in tokens
    "prompt_states": prompt_states,
}

# ---------------- SAVE ----------------
torch.save(data, SAVE_NAME)
print(f"âœ” Saved to {SAVE_NAME}")
